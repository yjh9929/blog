[[#**[1] Machine Learning의 3가지 중요한 요소 Data, Model, Learning**]]
[[#**[2] 머신러닝의 기본적인 용어들 정리**]]
[[#**[3] 책의 Part1. 머신러닝의 기초가 되는 수학**]]
[[#**[4] 책의 Part2. 머신러닝의 4가지 기둥**]]
### **[1] Machine Learning의 3가지 중요한 요소 : Data, Model, Learning**

**머신 러닝(Machine Learning, 기계학습, ML)** 은 컴퓨터 **스스로**(Automatically) 데이터에서 **유용한 정보**(information)를 뽑아내는 알고리즘을 디자인하는 것을 말한다. 여기서 **‘automatic(자동적)’**이라는 점을 강조한다. 즉, 머신러닝은 의미 있는 무언가를 생성하면서 많은 데이터셋에 적용할 수 있는 범용(general-purpose) 방법론(methodologies)을 연구한다. 

알고리즘이란, 어떠한 문제를 해결하는 방법을 정의한 일련의 단계적 절차를 의미하는데,  인공지능도 하나의 알고리즘이다. 여기서 기존의 다른 알고리즘이랑의 약간의 차이가 있는데, 다양하면서 많은 데이터에 범용적, 보편적으로 적용할 수 있는 알고리즘을 만든다는 점이다.

**머신 러닝에는 3가지 중요한 요소**가 있다. 바로 **Data(데이터), Model(모델), Learning(학습)** 이다.

#### 1. Data(데이터)

데이터는 머신러닝의 핵심이다. 머신러닝 자체가 데이터를 분석하는 것이다. 여기에 전처리 과정, 데이터를 해석하기 용이한 형태로 변환시키기는 과정도 포함된다. 기본적으로 데이터가 좋아야 분석 결과도 좋아진다. 지난 학기 Depth Estimation 하면서 여러 번 느낀 거지만, 데이터의 화질, 데이터를 얻을 때 했던 통제변인들이 모델을 만들 때 굉장히 중요했다.

#### 2. Model(모델)

머신러닝의 목표는 데이터를 통해 의미 있는 패턴을 추출하는 보편적인 방법을 설계하는 것이다. 이 목표를 달성하기 위해 데이터를 생성하는 모델을 설계한다. 데이터의 형태, 분석 목적에 맞는 적절한 모델을 선택해서 가져와야 한다.

모델에는 좋은 모델과 나쁜 모델이 있다. 좋은 모델은 데이터의 전체적이면서 일반적인 성향을 잘 보여주는 것을 말하고, 나쁜 모델은 연습용 데이터에 너무 적합해서 실전에서 사용하기 어렵거나 불필요한 부분을 반영했을 때를 말한다.

#### 3. Learning(학습)

학습과정은 모델의 매개변수를 최적화하여 데이터에서 패턴과 구조를 자동적으로 찾는 것을 의미한다.

> 솔직히 이렇게만 공부하면 간단해 보이지만, 그건 약간 억울하다. 머신러닝을 통해 완벽한 모델을 찾기 위해서는 학습과정을 정말 수도 없이 많이 거쳐 모델을 많이 생성해 내고, 모델들을 평가해 봐서 진짜 실생활에 쓸만한 친구인지 확인하고, 데이터도 다양하게 바꿔보는 등의 일을 한다. 

---

### **[2] 머신러닝의 기본적인 용어들 정리**

머신러닝의 개념들과 단어들이 굉장히 많기 때문에 헷갈리기 쉬운데, 정리하고 넘어가는 게 좋다. 사실 문맥에 따라 다른 의미를 갖기 때문에 주의해서 읽어야 한다.

#### 1. Predictor

머신러닝에서 알고리즘이라는 말이 두 가지 다른 상황에서 쓰인다. 첫 번째 머신러닝 알고리즘은 input data를 바탕으로 예측하는 시스템을 가리키는 경우이다. 이 시스템을 Predictor(예측)라고 한다.

#### 2. Training

두 번째 머신러닝 알고리즘은 Predictor 내부의 Parameters를 정하기 위한 과정이다. 이 과정은 Training(학습)이라 한다.

#### 3. Data as Vector

세상에는 데이터 중에는 비정형 데이터와 같이 분석하기 어려운 데이터들도 굉장히 많은데, 머신러닝에서는 **전처리가 완료(수치형으로 변환)**되어 분석에 용이하게 벡터로 표현되는 데이터들만 데이터라고 한다.

여기서 또 3가지 의미로 나눌 수 있는데,  
- 숫자들의 배열(Computer Science 관점)  
- 방향과 크기가 있는 화살표(Physics 관점)  
- 수학적 의미의 벡터(Mathematical 관점)

#### 4. Model

모델은 데이터 속에 숨겨진 패턴을 찾고 주어진 데이터를 바탕으로 미래를 예측하는 과정이다.

#### 5. Learning

많은 Training 방법들 중에서 단순히 현재 데이터를 가장 잘 설명하는 Parameters를 구하도록 되어 있는데, Learning이라는 개념은 좀 더 거시적으로 과최적화를 피하고 미래 데이터를 잘 예측하는 Parameters를 선택하는 과정이 포함된다.

---

### **[3] 책의 Part1. 머신러닝의 기초가 되는 수학**

이 책의 앞부분은 머신러닝의 기초가 되는 수학을 6단원으로 나눠서 설명한다. 참고로 각각의 챕터로 한 권의 책을 쓸 수 있을 정도로 방대한 내용을 냅다 달리기 때문에 흥미를 갖게 될 수는 있지만, 이해하기 어렵다. 솔직히 정리하기 좋은 내용은 아니다. 선배들도 깊게 이해하려면 너무 많은 내용들을 참고해야 한다고 했다.

#### Ch2. Linear Algebra 선형 대수학

수치 데이터를 벡터로 표현하고, 이런 데이터의 표는 행렬로 표현한다. 벡터와 행렬에 대한 연구를 선형 대수학이라고 한다. Ch3과 4까지 선형 대수학 내용이 이어진다.

#### Ch3. Analytics Geometry 해석 기하학

만약 2개의 행렬이 현실 세계에서 두 개의 유사한 객체를 낸다고 가정해 보자. 그러면 두 개의 행렬이 머신러닝을 통해 나온 결과는 유사할 것으로 예상할 수 있다. 이 두 벡터가 얼마나 유사성(similarity)을 띄는지 결과를 '거리(distance)'를 숫자 값으로 표현할 수 있다. 이러한 역할을 하는 분야가 바로 해석 기하학이다.

#### Ch4. Matrix Decompositions 행렬 분해

4장에서는 행렬과 행렬 분해에 대한 몇 가지 기본 개념을 소개한다. 어떠한 xy 평면 위의 점에 대해서 x좌표와 y좌표로 나타낼 수 있는데, (x, y) = (x, 0) + (0, y) = x(1, 0) + y(0, 1)로 나타낼 수 있다(span). 여기서 (1, 0)과 (0 ,1)라는 벡터에 적당한 숫자를 곱해서 나타내는 것처럼 기초적은 벡터(basis)를 통해 모든 벡터를 다 표현할 수 있다.

#### Ch5. Vector Calculus 벡터 계산

5장은 7장을 위한 전초전이다. 머신러닝에서는 데이터가 가지고 있는 다양한 속성을 측정하는데, 일부 성능을 최대화하는 매개 변수를 찾는다. 많은 최적화 기술에는 해법(solution)을 찾는 방향을 알려주는 그래디언트(Gradient)의 개념이 필요한데, 5장에서는 벡터 미적분학에 관한 것이며, 기울기의 개념을 설명한다.

#### Ch6. Probability and Distributions 확률과 분포

데이터를 관측하다 보면 기본 데이터에 잡음 데이터가 섞여있는 것을 볼 수 있다. 이때 noisy observation을 제거하고 싶을 때 noise를 정량화해야 한다. 어떤 게 noise이고, 어떻게 구분해야 하는지. 또한 평소와는 다른 불확실성을 표현해야 하는 경우도 있는데, 이러한 확률적인 부분을 다루는 장이다.

#### Ch7. Continuous Optimization 최적화

5장에 이어 함수의 maxima(최댓값), minima(최솟값)를 찾기 위한 최적화를 다룬다.

---

### **[4] 책의 Part2. 머신러닝의 4가지 기둥**

여기서부터는 수학적인 내용보다는 앞선 내용들을 가지고 머신러닝의 내용을 이야기하는 내용들이 나온다. 한 장 한 장 이해하기 엄청 어렵다. 아마 앞부분은 후루룩 정리한다 해도, 뒷부분은 일주일에 하나 하기도 벅찰 듯... 그럼 중간고사 전에 끝나긴 하려나?

#### Ch8. When Models Meet Data

8장은 앞으로 이야기할 내용의 인트로에 가깝다.

#### Ch9. Linear Regression 선형 회귀 분석

머신러닝의 가장 기본적인 예측 모델이 선형 회귀 분석을 다룬다. 주어진 입력값들이 있을 때, 예측값 y들로 보내는 함수를 찾는다. 이러한 y를 x의 레이블이라고 한다. 매개변수를 최적화하는 대신 통합하는 베이지안 선형 회귀뿐만 아니라 likelihood와 maximum a posteriori estimation에 대해서도 공부한다.

#### Ch10. Dimensionality Reduction with Principal Component Analysis 차원 축소

차원 축소란 복잡한 데이터를 좀 더 분석을 쉽게 하기 위해 줄이는 것을 말한다. 주요한 정보가 아닌 정보들은 줄이고 확인하는 과정이라고 할 수 있다.  입력값과 함숫값(레이블)을 모두 살펴보는 회귀분석과 다르게 입력값 x에만 집중한다.

#### Ch11. Density Estimation with Gaussian Mixture Models 밀도 추정

주어진 데이터셋을 설명하는 확률분포를 찾는 것을 말한다. 이를 위해 혼합 가우시안 모델에 대해 좀 더 다룬다. 여기서 가우시안 분포란 정규분포를 말한다. 차원축소처럼 x에 대한 레이블은 없다. 하지만 저차원적 표현을 추구하지 않는다는 점에서 다르다.

#### Ch12. Classification with Support Vector Machines 분류

데이터 분류를 서포트벡터의 맥락에서 배운다. 회귀와 유사하게 입력과 해당 레이블 y가 있다. 하지만 레이블이 실수값인 회귀와 다르게 레이블이 정수값이다.
